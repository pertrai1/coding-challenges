# TDD Pair Programming Session

You are the orchestrator for a TDD pair programming session. You coordinate agents to solve a LeetCode problem using strict Test-Driven Development.

Follow the `tdd-pair-loop` skill methodology exactly.

## Session Modes

This command supports two modes:

### Auto-Solve Mode (default)

Three agents â€” Driver (test writer), Navigator (code writer), and Researcher (read-only context) â€” collaborate to solve the problem autonomously.

Agents: `.claude/agents/tdd-driver.md`, `.claude/agents/tdd-navigator.md`, `.claude/agents/tdd-researcher.md`

### Coach Mode (`--mode=coach`)

The **learner writes all implementation code**. Two agents support them â€” Driver (test writer) and Coach (guided hints). The Researcher is available on request.
Coach mode also uses a **progressive test-ownership ladder** so the learner gradually takes on test design and then test authoring.

Agents: `.claude/agents/tdd-driver.md`, `.claude/agents/tdd-coach.md`, `.claude/agents/tdd-researcher.md`

**To start coach mode**, the user must include `--mode=coach` in their command or explicitly ask for guided/coach/learning mode.

In coach mode, the learner learns by:

1. Reading TODO comments the Coach inserts as hints
2. Stating the pattern and invariant before coding
3. Writing the implementation code themselves
4. Progressively taking ownership of tests (from proposing cases to writing full `it()` blocks)
5. Getting interview-style feedback on request

## Session Flow

Execute these phases in order. Do not skip any phase.

### Phase 1: Problem Selection & Setup

1. **Select a problem.** If the user provided a specific problem, use that. Otherwise:
   - Ask the user for difficulty preference (easy / medium / hard / random) and category preference (Two Pointers, Sliding Window, Breadth-First Search, Depth-First Search, Backtracking, Heap, Binary Search, Dynamic Programming, Divide and Conquer, Trie, Union Find, Greedy, or random)
   - Search the web for a LeetCode problem matching the criteria
   - Cross-reference against existing directories in `leetcode/` to confirm it's not already solved
   - If the problem is already solved, pick another one
   - Present the problem to the user and get confirmation before proceeding

2. **Read the problem statement.** Get the full problem description, examples, and constraints from LeetCode.

3. **Create the problem directory and files:**
   - Determine the difficulty folder: `leetcode/easy/`, `leetcode/medium/`, or `leetcode/hard/`
   - Create the directory: `leetcode/{difficulty}/{4-digit-number}-{kebab-case-name}/`
   - Create `README.md` with the problem statement using **pure markdown formatting** (no HTML elements). Convert the LeetCode HTML to markdown:
     - Use `#` headings instead of `<p><strong>`
     - Use `**bold**` instead of `<strong>`, `*italic*` instead of `<em>`
     - Use `` `backticks` `` instead of `<code>`
     - Use markdown code blocks (```) instead of `<pre>`
     - Use markdown lists (`-`) instead of `<ul><li>`
     - Use `^` for superscripts where needed (e.g., `10^4` instead of `<sup>4</sup>`)
     - Format: title with LeetCode link, difficulty badge, markdown problem description, examples, constraints
     - Run `npx markdownlint {README-file}` to verify â€” fix any violations before proceeding
   - Create the implementation file `{name}.ts` with a stub export:

     ```typescript
     export function functionName(): void {
       throw new Error('Not implemented');
     }
     ```

   - Create the test file `{name}.test.ts` with a stub describe block:

     ```typescript
     import { describe, it, expect } from 'vitest';
     import { functionName } from './{name}';

     describe('{Problem Name}', () => {
       // Tests will be added one at a time by the Driver
     });
     ```

4. **Create a git branch:** `tdd/{number}-{name}`
   - First ensure you're on `main` and it's clean
   - Create and checkout: `git checkout -b tdd/{number}-{name}`

### Phase 2: TDD Loop

Run the red-green-refactor cycle. The steps differ based on the session mode.

---

#### Auto-Solve Mode (Phase 2)

##### Step A: Driver Turn (RED)

Delegate to the Driver agent (use `delegate_task` with the `tdd-driver` agent definition or act as the Driver role):

- **Context to provide**: Problem statement, test file path, implementation file path, cycle number, and any previous HANDOFF from the Navigator
- **Expected output**: A HANDOFF block with Phase=RED, the new test, and vitest FAIL output
- **Verify**: The HANDOFF includes actual test failure output. If not, re-run the test.

##### Step B: Researcher Turn (CONDITIONAL)

Only if the Driver's HANDOFF contains `Research needed:` with a value other than "none":

- Delegate to the Researcher agent
- **Context to provide**: The research question, problem statement, and relevant paths
- **Expected output**: Concise research findings
- Pass the findings to the Navigator in the next step

##### Step C: Navigator Turn (GREEN)

Delegate to the Navigator agent (use `delegate_task` with the `tdd-navigator` agent definition or act as the Navigator role):

- **Context to provide**: Driver's HANDOFF, implementation file path, test file path, researcher findings (if any), and cycle number
- **Expected output**: A HANDOFF block with Phase=GREEN, the implementation change, and vitest PASS output
- **Verify**: ALL tests pass. If any test fails, send the Navigator back to fix it before proceeding.
- **Commit**: Immediately commit the progress to enable "TDD Playback" in the PR:

  ```bash
  git add .
  git commit -m "test: cycle {cycle_number} (Green)"
  ```

##### Step D: Loop Decision

Check Navigator's HANDOFF:

- If `Next step` says "ready for next test" â†’ go to Step A with the next cycle number
- If `Next step` says "DONE" â†’ check completion criteria below
- If Driver signals `DONE` in their next turn â†’ proceed to Phase 3

**Completion criteria** (ALL must be true):

- All example cases from the problem have tests
- At least 2-3 edge cases are tested
- All tests pass
- At least 4 TDD cycles have been completed

If criteria aren't met, continue the loop.

---

#### Coach Mode (Phase 2)

In coach mode, the learner writes all implementation code. Test ownership progresses by cycle so verification skills improve without overloading early cycles.

##### Coach Mode Test-Ownership Ladder (default)

- **Level 1 â€” Test intent (cycles 1-2):** Learner proposes the next test case and expected output. Driver writes the full Vitest `it()` block.
- **Level 2 â€” Assertion sketch (cycles 3-4):** Learner provides concrete input/output and assertion shape. Driver finalizes syntax and structure.
- **Level 3 â€” Learner-authored test (cycle 5+):** Learner writes the full `it()` block. Driver only reviews for clarity and runs RED.
- **Progression rule:** Advance to next level when (a) cycle number reaches threshold AND (b) learner has shown confidence at current level
- **Stay-at-level indicators:** Learner struggled significantly at current level, or session will end before reaching cycle 5
- **De-escalation rule:** If the learner is stuck or severely time-boxed, drop one level for a single cycle, then resume progression.

##### Step A: Driver Turn (RED)

Delegate to the Driver agent with the active test-ownership level:

- **Context to provide**: Problem statement, test file path, implementation file path, cycle number, active test-ownership level, learner's test contribution for this cycle, and any previous HANDOFF from the Coach
- **Expected output**: A HANDOFF block with Phase=RED, the new test, vitest FAIL output, and how the learner contributed to the test at this level
- **Verify**: The HANDOFF includes actual test failure output. If not, re-run the test.

##### Step B: Researcher Turn (CONDITIONAL)

Same as auto-solve mode. Only triggered if `Research needed:` is not "none".

##### Step C: Coach Turn (GUIDE)

Delegate to the Coach agent (use `delegate_task` with the `tdd-coach` agent definition or act as the Coach role):

- **Context to provide**: Driver's HANDOFF, implementation file path, test file path, researcher findings (if any), cycle number, and the learner's previous code (if any)
- **The Coach will**:
  1. Insert 1-3 TODO comments in the implementation file describing what must become true
  2. Ask the learner the **pattern checkpoint question**: "What pattern are you using, what are the key state variables, and what invariant must hold?"
  3. Wait for the learner to respond with their pattern statement
  4. Evaluate the pattern statement:
     - **If correct and complete:** Tell the learner to go ahead and implement
     - **If partially correct:** Use targeted follow-up questions:
       - "You mentioned [X]. What about [Y state variable]?"
       - "That's the right pattern. What specific data do you need to track?"
       - "Good start. What property must remain true at each step?"
     - **If incorrect:** Guide them with questions (do NOT give the answer)
  5. Record the test-ownership level and learner test contribution for the cycle in Coach HANDOFF notes

##### Step D: Learner Codes

**The learner writes the implementation code** (and, on Level 3 cycles, has already authored the new test in Step A). The orchestrator waits for the learner to respond with one of:

- **"continue"** â€” The Coach reviews their code, runs tests, and either:
  - **If tests pass:**
    - Remove completed TODO comments from the implementation file
    - Acknowledge what was done well (1 sentence max)
    - **Ask complexity question:** "What's the time/space complexity of what you just implemented and why?"
    - Record their answer in HANDOFF for interview evidence (don't correct if slightly off, but note it for the debrief)
    - **Optionally suggest refinements** (only if code works correctly):
      - Combining related loops (e.g., initialization + counting in same pass)
      - Removing unused data from structures (e.g., extra fields in queue items)
      - Adding early returns for base cases
      - **Rule:** Never make the learner feel their working code is "wrong" â€” these are optional improvements
    - Commit progress immediately
    - Set/confirm the next cycle's test-ownership level and tell the learner what test contribution is expected next
    - Insert the next 1-3 TODO comments for the next behavioral step
    - Ask the pattern checkpoint question
  - **If tests fail:**
    - Show the failure and ask a guiding question (if tests fail)
- **"feedback"** â€” The Coach reads the learner's code changes first, runs tests, then pauses progression and delivers interview-style evaluation based on what the learner actually wrote:
  - Scores 5 categories (Problem Solving, Coding, Verification, Communication, Complexity Analysis) on a 1-5 scale
  - Provides concrete observations quoting specific lines from the learner's code
  - Acknowledges what the learner got right before critiquing
  - Identifies one prioritized improvement target
  - Learner says "continue" when ready to resume

##### Step E: Loop Decision

Check Coach's HANDOFF:

- If `Next step` indicates more TODOs needed â†’ go to Step A (next test is created per active ownership level)
- If `Next step` says "DONE" â†’ check completion criteria below

**Debugging Cycles:** When a cycle is primarily fixing a bug (not adding new behavior):

- Use hint level 1 (guiding questions about what variables should be)
- Don't advance test-ownership level (stay at current L1/L2/L3)
- In HANDOFF, mark cycle type as "debugging cycle" in notes
- Still commit progress after fix with descriptive message (e.g., "fix freshCount initialization")

**Completion criteria** (ALL must be true):

- All example cases from the problem have tests
- At least 2-3 edge cases are tested
- All tests pass
- The learner wrote all implementation code (Coach only inserted TODO comments)
- The learner completed at least one cycle at test-ownership Level 2+ (preferably at least one Level 3 cycle)
- At least 4 TDD cycles have been completed

##### Coach Mode Commit Strategy

**Commit immediately after tests pass:**

```bash
git add .
git commit -m "learn: cycle {cycle_number} - {brief description of what learner implemented}"
```

**Why this matters:**

- Creates a "Learning Playback" history showing incremental progress
- Prevents lost work if issues arise
- Demonstrates TDD rhythm (test â†’ implement â†’ commit)
- Provides restore points if learner wants to try different approaches

**Commit message tips:**

- Cycle 1: "count fresh oranges and return 0 if none"
- Cycle 3: "implement BFS with level-by-level processing"
- Cycle 4 (debugging): "fix freshCount initialization to count fresh oranges"

This enables "Learning Playback" in the PR â€” reviewers can see how the learner built the solution step by step.

### Phase 3: Post-Mortem

The post-mortem format differs based on session mode.

#### Auto-Solve Mode Post-Mortem

1. Copy `POST_MORTEM_TEMPLATE.md` to the problem directory as `POST_MORTEM.md`
2. Fill out the post-mortem as an expert algorithm reviewer. Use the `review-solution` skill methodology:
   - **Problem**: Describe in your own words, include problem name and LeetCode link
   - **Time Tracking**: Estimate based on the number of TDD cycles
   - **Solution Exploration**: All approaches considered (including early naive implementations from TDD), final complexity analysis, trade-offs
   - **Pattern Recognition**: Algorithmic pattern, key insight ("aha moment"), 2-3 related problems
   - **Edge Cases**: What was handled, what was missed
   - **Retrospective**: Key takeaways from the TDD process
   - **Rubric**: Self-rate the solution (1-5 scale)

#### Coach Mode Post-Mortem (Interview Debrief)

In coach mode, do NOT use `POST_MORTEM_TEMPLATE.md`. Instead, write the post-mortem as a **senior software engineer who just concluded a coding interview with the learner**. Write from the interviewer's perspective (for example, "I observed...", "I would move forward..."), never as learner self-reflection. Keep the tone evaluative, professional, and supportive. Reference specific moments from the session.

Create `POST_MORTEM.md` in the problem directory with this structure:

```markdown
# Interview Debrief â€” {Problem Name}

**Date:** {date}
**Problem:** [{Problem Name}](https://leetcode.com/problems/{slug}/) â€” {Difficulty}
**Duration:** ~{estimated time} minutes
**Mode:** Guided TDD (Coach Mode)

**Duration Estimation:**

- If you tracked cycle timestamps, use actual duration
- Otherwise estimate: (number of cycles Ã— 15-25 minutes per cycle)
- Coach mode typically takes longer than auto-solve due to teaching pauses
- Note the duration is for the coding session, not including problem reading/setup

---

## Session Overview

{2-3 sentence summary of how the session went. Set the tone â€” was the candidate
strong, improving, struggling? What stood out?}

---

## Problem Solving

**Score: {1-5}/5**

**Strengths:**

- {What the learner did well in identifying patterns, choosing approaches}

**Areas for Improvement:**

- {Where the learner struggled, needed hints, or missed the pattern}

**Help Needed:** {Summary of hint levels used â€” e.g., "Minimal â€” one L1 guiding
question" or "Significant â€” needed L2 pattern hints on cycles 2 and 4"}

---

## Coding

**Score: {1-5}/5**

**Strengths:**

- {Code quality, variable naming, structure, TypeScript usage}

**Areas for Improvement:**

- {Syntax issues, logic errors, unnecessary complexity, debugging struggles}

**Debugging Process:**

- {How did they respond when tests failed? Systematic or flailing?}

---

## Verification

**Score: {1-5}/5**

**Strengths:**

- {Edge case awareness, invariant understanding, proactive testing}

**Areas for Improvement:**

- {Missed edge cases, didn't verify invariants, didn't trace through examples}

---

## Communication

**Score: {1-5}/5**

**Strengths:**

- {Pattern checkpoint responses, articulation of invariants, questions asked}

**Areas for Improvement:**

- {Difficulty explaining approach, unclear invariant statements, silence during
  struggle}

---

## Complexity Analysis

**Score: {1-5}/5**

**Strengths:**

- {Accuracy of time/space analysis, reasoning quality}

**Areas for Improvement:**

- {Incorrect analysis, missing edge case impact, no amortized reasoning}

---

## Pattern Recognition

- **Pattern Used:** {e.g., Variable-Size Sliding Window}
- **Key Insight:** {The "aha moment" â€” when did they get it?}
- **Pattern Checkpoint Results:** {X/Y correct on first attempt}
- **Related Problems to Practice:**
  - {Problem 1 â€” why it reinforces the same pattern}
  - {Problem 2 â€” a harder variant}
  - {Problem 3 â€” same pattern, different domain}

---

## Session Progression

{Brief narrative of how the learner progressed through the cycles. Reference
specific cycles where breakthroughs or struggles happened.}

| Cycle | What Was Tested | Test Ownership Level | Hint Level | Pattern Checkpoint | Notes        |
| ----- | --------------- | -------------------- | ---------- | ------------------ | ------------ |
| 1     | {test name}     | {L1/L2/L3}           | {0-3}      | {PASS/FAIL}        | {brief note} |
| 2     | {test name}     | {L1/L2/L3}           | {0-3}      | {PASS/FAIL}        | {brief note} |
| ...   | ...             | ...                  | ...        | ...                | ...          |

---

## No-Hire Trigger Check

- **Critical Triggers Observed:** {none | list triggers with cycle references}
- **Recovery Evidence:** {none needed | how candidate recovered from each triggered item}
- **Guardrail Applied:** {none | "cap at Lean No Hire" | "No Hire due to unresolved critical trigger"}

Use these triggers as interview guardrails:

- Could not explain the core invariant after coaching attempts
- Needed Level 3 hints for core algorithm logic more than once
- Could not explain final time/space complexity accurately
- Could not justify why the final solution is correct
- Repeated the same failing assertion 3+ times without a new debugging strategy
- Left a critical correctness/edge-case bug unresolved at session end

Guardrail rules:

- If 2+ critical triggers occur without clear recovery, max Hiring Decision is **Lean No Hire**
- If any critical trigger remains unresolved by session end, set Hiring Decision to **No Hire** and Would Move Forward to **No**

---

## Overall Assessment

**Interview Recommendation:** {Recommend / Lean Recommend / Neutral / Lean No / Do Not Recommend}
**Hiring Decision:** {Strong Hire / Hire / Lean Hire / Lean No Hire / No Hire}
**Would Move Forward to Next Round:** {Yes / No}
**Confidence:** {High / Medium / Low}

**Summary:**
{2-3 sentences on overall performance. State directly whether you would hire or
pass on the candidate and why, based on session evidence.}

**Strengths to Build On:**

- {Top 2-3 things the learner should keep doing}

**Priority Areas for Growth:**

- {Top 2-3 things to work on, in order of impact}

**Recommended Next Steps:**

- {Specific problems to practice that target their weak areas}
- {Specific pattern or technique to study}
- {Any behavioral change â€” e.g., "trace through examples before coding"}

---
```

The tone should match a real interview debrief: professional, specific, evidence-based, and constructive. Reference actual code the learner wrote, actual pattern checkpoint responses, and actual hint escalation moments. Do NOT be generic â€” every observation should be tied to something that happened in the session. Do NOT write this as the learner ("I learned", "I struggled"). Write it as the interviewer evaluating the learner. Apply the no-hire trigger guardrails explicitly in the final decision.

#### Quality Checks (Both Modes)

1. Run quality checks on all files in the problem directory:
   - `npx eslint {implementation-file}` â€” fix any issues
   - `npx prettier --write {implementation-file} {test-file}` â€” format code
   - `npx markdownlint leetcode/{difficulty}/{number}-{name}/README.md` â€” verify markdown quality, fix any violations
   - `npx markdownlint leetcode/{difficulty}/{number}-{name}/POST_MORTEM.md` â€” verify markdown quality, fix any violations
   - `npx vitest run {test-file}` â€” final confirmation all tests pass

### Phase 4: Update Problem Index

Update `docs/PROBLEMS.md` to include the new problem:

1. **Add the problem link** to the correct difficulty section, in the appropriate category subsection:
   - Format: `- [{number} - {Problem Name}](../leetcode/{difficulty}/{number}-{name}) ![{Difficulty}](https://img.shields.io/badge/{Difficulty}-{color})`
   - Colors: Easy=green, Medium=orange, Hard=red
   - Place alphabetically or by problem number within the subsection
   - If no existing subsection fits, add it to the most relevant one or create a new subsection

2. **If the problem matches a pattern** listed in "Problems by Pattern", add it there too.

3. **Run markdownlint**: `npx markdownlint docs/PROBLEMS.md` â€” fix any violations before proceeding.

### Phase 5: Pull Request

1. Stage all changed files:

   ```bash
   git add leetcode/{difficulty}/{number}-{name}/ docs/PROBLEMS.md
   ```

2. Commit:

   ```bash
   # Auto-solve mode:
   git commit -m "feat: solve {Problem Name} (#{number}) via TDD"

   # Coach mode:
   git commit -m "learn: solve {Problem Name} (#{number}) via guided TDD"
   ```

3. Push:

   ```bash
   git push -u origin tdd/{number}-{name}
   ```

4. Create the pull request. Use the appropriate template based on session mode:

   **Auto-solve mode:**

   ```bash
   gh pr create \
     --title "feat: solve {Problem Name} (#{number})" \
     --body "## Problem
   [{Problem Name}](https://leetcode.com/problems/{slug}/) â€” {Difficulty}

   ## Approach
   {Brief summary of the final algorithm from the post-mortem}

   ## Complexity
   - **Time**: O(...)
   - **Space**: O(...)

   ## Pattern
   {Algorithmic pattern used}

   ## TDD Summary
   - **Cycles**: {number of red-green cycles}
   - **Tests**: {number of test cases}
   - **Key Insight**: {the aha moment from the post-mortem}

   ## Self-Review Questions
   1. {Question about key logic or trade-off}
   <details>
     <summary>Answer</summary>
     {Answer}
   </details>
   2. {Question about edge case or complexity}
   <details>
     <summary>Answer</summary>
     {Answer}
   </details>

   ## Files
   - \`README.md\` â€” Problem statement
   - \`{name}.ts\` â€” Solution
   - \`{name}.test.ts\` â€” Test suite
   - \`POST_MORTEM.md\` â€” Analysis and retrospective" \
     --label "code challenge"
   ```

   **Coach mode:**

   ```bash
   gh pr create \
     --title "learn: solve {Problem Name} (#{number}) via guided TDD" \
     --body "## Problem
   [{Problem Name}](https://leetcode.com/problems/{slug}/) â€” {Difficulty}

   ## Approach
   {Brief summary of the final algorithm from the post-mortem}

   ## Complexity
   - **Time**: O(...)
   - **Space**: O(...)

   ## Pattern
   {Algorithmic pattern used}

   ## Learning Summary
   - **Mode**: Coach (learner wrote all implementation code)
   - **Cycles**: {number of guided cycles}
   - **Tests**: {number of test cases}
   - **Hint Levels Used**: {summary â€” e.g., 'Mostly L1, L2 on cycle 3'}
   - **Key Insight**: {the aha moment from the post-mortem}
   - **Pattern Checkpoints**: {how many PASS vs FAIL â€” e.g., '5/7 correct on first attempt'}

   ## ðŸ§  Self-Review Questions
   1. {Question about key logic or trade-off}
   <details>
     <summary>Answer</summary>
     {Answer}
   </details>
   2. {Question about edge case or complexity}
   <details>
     <summary>Answer</summary>
     {Answer}
   </details>

   ## Files
   - \`README.md\` â€” Problem statement
   - \`{name}.ts\` â€” Solution
   - \`{name}.test.ts\` â€” Test suite
   - \`POST_MORTEM.md\` â€” Analysis and retrospective" \
     --label "code challenge"
   ```

5. **Add Educational Annotations (Agent-Annotated PR)**:
   - Act as the **DSA Code Reviewer** agent.
   - Analyze the final solution code in `{name}.ts`.
   - Identify 2-3 specific blocks of code that demonstrate the "Key Insight" or interesting patterns.
   - For each block, post a review comment on the PR (or a general comment if line-targeting is difficult):

     ```bash
     gh pr comment {PR_URL} --body "ðŸ’¡ **Educational Note**: {Explanation of why this code is written this way, complexity analysis, or pattern info}"
     ```

6. **Add Engaging Code Explanation Comment**:
   - Analyze the final solution code in `{name}.ts`.
   - Write a comment that explains the code in the function and how it solves the problem.
   - The explanation should be very engaging to read â€” don't make it sound like boring technical documentation or a textbook. Where appropriate, use analogies and anecdotes to make it more understandable and memorable.
   - Post the explanation as a PR comment:

     ```bash
     gh pr comment {PR_URL} --body "## ðŸŽ¯ How This Solution Works

     {Engaging explanation of the code â€” use analogies, anecdotes, and a conversational tone to make the algorithm come alive}"
     ```

7. Report the PR URL to the user.

## Important Notes

- **Always run tests in single-run mode**: `npx vitest run` (never watch mode)
- **TypeScript**: All solution and test files use `.ts` extension
- **Imports**: Use named exports from implementation file, import in test file
- **Vitest globals**: The config has `globals: true`, but still import `describe`, `it`, `expect` explicitly for clarity
- **Existing conventions**: Match the naming pattern of existing problems (4-digit number, kebab-case)
- **No external dependencies**: Solutions must be self-contained (no lodash, etc.)
- **Markdown only**: All `.md` files must use pure markdown formatting â€” no HTML elements like `<p>`, `<code>`, `<strong>`, `<em>`, `<pre>`, `<ul>`, `<li>`. The markdownlint config (`.markdownlint.json`) allows only `<details>`, `<summary>`, `<sup>`, `<sub>`, `<br>`, `<img>`.
- **markdownlint**: Run `npx markdownlint {file}` on every markdown file created or modified. Fix violations before committing.
- **Problem index updates**: `docs/PROBLEMS.md` must be updated with the new problem link every time a new problem is solved.

### Coach Mode Specific Notes

- **The learner writes all implementation code.** The Coach inserts TODO comments only.
- **Pattern checkpoints are mandatory.** The learner must state the pattern and invariant before coding each cycle.
- **Test ownership is progressive.** In coach mode, use the L1â†’L2â†’L3 ladder so the learner gradually owns more of test design and writing.
- **TODO comments are temporary.** Remove them after the corresponding tests pass. The final solution should have no instructional comments.
- **Hint escalation is progressive.** Start at Level 1 (guiding questions), only escalate to Level 2/3 when the learner is demonstrably stuck.
- **"feedback" pauses progression.** The learner must say "continue" to resume after receiving feedback.
- **Coach may fix syntax but not algorithm.** Missing semicolons or unclosed brackets are fair game; writing loop logic or conditionals is not.
- **Post-mortem in coach mode** is an interview debrief, not a self-reflection. Write it as a senior engineer evaluating a candidate from the interviewer's perspective. Include hint levels per cycle, pattern checkpoint pass rate, a cycle-by-cycle progression table, no-hire trigger check, an interview recommendation, and an explicit hiring decision (Strong Hire/Hire/Lean Hire/Lean No Hire/No Hire) with move-forward call and confidence.
